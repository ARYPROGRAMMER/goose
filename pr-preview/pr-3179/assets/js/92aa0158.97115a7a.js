/*! For license information please see 92aa0158.97115a7a.js.LICENSE.txt */
(self.webpackChunkgoose=self.webpackChunkgoose||[]).push([[3186],{46792:(e,t,n)=>{"use strict";n.d(t,{$:()=>r});n(96540);var i=n(74848);const r=e=>{let{children:t,className:n="",variant:r="default",size:a="default",...s}=e;return(0,i.jsx)("button",{className:`flex rounded-full focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-accent dark:focus:ring-offset-gray-900 ${{default:"bg-black dark:bg-white text-white dark:text-black hover:bg-accent/90 dark:hover:bg-accent/80",ghost:"bg-transparent hover:bg-gray-100 dark:hover:bg-gray-700 dark:text-gray-300",link:"bg-transparent text-accent hover:underline hover:text-textProminent dark:text-accent/90"}[r]} ${{default:"px-6 py-3",icon:"p-2"}[a]} ${n}`,...s,children:t})}},57356:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>f});var i=n(43938),r=n(16445),a=n(56347),s=n(96540),o=n(56289),l=n(52362),c=n(58069),d=n(46792),u=n(54631),p=n(62636),h=n(74848);const m={"GitHub MCP":"bg-yellow-100 text-yellow-800 border-yellow-200","Context7 MCP":"bg-purple-100 text-purple-800 border-purple-200",Memory:"bg-blue-100 text-blue-800 border-blue-200"};function f(){const e=(0,a.zy)(),[t,n]=(0,s.useState)(null),[f,g]=(0,s.useState)(!0),[y,b]=(0,s.useState)(null),[v,w]=(0,s.useState)(!1),[x,k]=(0,s.useState)({});(0,s.useEffect)((()=>{(async()=>{try{g(!0),b(null),k({}),w(!1);const t=new URLSearchParams(e.search).get("id");if(!t)return void b("No recipe ID provided");const i=await(0,u.d)(t);i?n(i):b("Recipe not found")}catch(t){b("Failed to load recipe details"),console.error(t)}finally{g(!1)}})()}),[e]);const _=t?.parameters||[];_.filter((e=>"required"===e.requirement));if(f)return(0,h.jsx)(i.A,{children:(0,h.jsx)("div",{className:"min-h-screen flex items-start justify-center py-16",children:(0,h.jsxs)("div",{className:"container max-w-5xl mx-auto px-4 animate-pulse",children:[(0,h.jsx)("div",{className:"h-12 w-48 bg-bgSubtle dark:bg-zinc-800 rounded-lg mb-4"}),(0,h.jsx)("div",{className:"h-6 w-full bg-bgSubtle dark:bg-zinc-800 rounded-lg mb-2"}),(0,h.jsx)("div",{className:"h-6 w-2/3 bg-bgSubtle dark:bg-zinc-800 rounded-lg"})]})})});if(y||!t)return(0,h.jsx)(i.A,{children:(0,h.jsx)("div",{className:"min-h-screen flex items-start justify-center py-16",children:(0,h.jsx)("div",{className:"container max-w-5xl mx-auto px-4 text-red-500",children:y||"Recipe not found"})})});const j="string"==typeof t.author?t.author:t.author?.contact;return(0,h.jsxs)(i.A,{children:[(0,h.jsx)("div",{className:"min-h-screen py-12",children:(0,h.jsxs)("div",{className:"max-w-4xl mx-auto px-4",children:[(0,h.jsxs)("div",{className:"mb-8 flex justify-between items-start",children:[(0,h.jsx)(o.A,{to:"/recipes",children:(0,h.jsxs)(d.$,{className:"flex items-center gap-2",children:[(0,h.jsx)(r.A,{className:"h-4 w-4"}),"Back"]})}),j&&(0,h.jsxs)("a",{href:`https://github.com/${j}`,target:"_blank",rel:"noopener noreferrer",className:"flex items-center gap-2 text-sm text-textSubtle hover:underline",children:[(0,h.jsx)("img",{src:`https://github.com/${j}.png`,alt:j,className:"w-6 h-6 rounded-full"}),"@",j]})]}),(0,h.jsxs)("div",{className:"bg-white dark:bg-[#1A1A1A] border border-borderSubtle dark:border-zinc-700 rounded-xl p-8 shadow-md",children:[(0,h.jsx)("h1",{className:"text-4xl font-semibold mb-2 text-textProminent dark:text-white",children:t.title}),(0,h.jsx)("p",{className:"text-textSubtle dark:text-zinc-400 text-lg mb-6",children:t.description}),t.activities?.length>0&&(0,h.jsxs)("div",{className:"mb-6 border-t border-borderSubtle dark:border-zinc-700 pt-6",children:[(0,h.jsx)("h2",{className:"text-2xl font-medium mb-2 text-textProminent dark:text-white",children:"Activities"}),(0,h.jsx)("div",{className:"flex flex-wrap gap-2",children:t.activities.map(((e,t)=>(0,h.jsx)("span",{className:"bg-surfaceHighlight dark:bg-zinc-900 border border-border dark:border-zinc-700 rounded-full px-3 py-1 text-sm text-textProminent dark:text-zinc-300",children:e},t)))})]}),t.extensions?.length>0&&(0,h.jsxs)("div",{className:"mb-6 border-t border-borderSubtle dark:border-zinc-700 pt-6",children:[(0,h.jsx)("h2",{className:"text-2xl font-medium mb-2 text-textProminent dark:text-white",children:"Extensions"}),(0,h.jsx)("div",{className:"flex flex-wrap gap-2",children:t.extensions.map(((e,t)=>{const n="string"==typeof e?e:e.name;return(0,h.jsx)("span",{className:`border rounded-full px-3 py-1 text-sm ${m[n]||"bg-gray-100 text-gray-800 border-gray-200 dark:bg-zinc-900 dark:text-zinc-300 dark:border-zinc-700"}`,children:n},t)}))})]}),t.prompt&&(0,h.jsxs)("div",{className:"mb-6 border-t border-borderSubtle dark:border-zinc-700 pt-6",children:[(0,h.jsx)("h2",{className:"text-2xl font-medium mb-4 text-textProminent dark:text-white",children:"Initial Prompt"}),(0,h.jsx)(l.A,{type:"info",className:"mb-4",children:"This prompt auto-starts the recipe when launched in Goose."}),(0,h.jsx)(c.A,{language:"markdown",children:t.prompt})]}),t.instructions&&(0,h.jsxs)("div",{className:"mb-6 border-t border-borderSubtle dark:border-zinc-700 pt-6",children:[(0,h.jsx)("h2",{className:"text-2xl font-medium mb-4 text-textProminent dark:text-white",children:"Instructions"}),(0,h.jsx)(c.A,{language:"markdown",children:t.instructions})]}),(0,h.jsxs)("div",{className:"pt-8 border-t border-borderSubtle dark:border-zinc-700 mt-6 flex gap-4",children:[(0,h.jsx)(o.A,{to:t.recipeUrl,target:"_blank",className:"inline-block text-white bg-black dark:bg-white dark:text-black px-6 py-2 rounded-full text-sm font-medium hover:bg-gray-900 dark:hover:bg-gray-100 transition-colors",children:"Launch in Goose Desktop \u2192"}),(0,h.jsxs)("div",{className:"relative group inline-block",children:[(0,h.jsx)("button",{onClick:()=>{if(_.length>0)return k({}),void w(!0);const e=`goose run --recipe ${t?.localPath}`;navigator.clipboard.writeText(e),p.Ay.success("CLI command copied!")},className:"text-sm font-medium px-6 py-2 rounded-full bg-zinc-200 dark:bg-zinc-800 text-zinc-700 dark:text-white hover:bg-zinc-300 dark:hover:bg-zinc-700 transition-colors cursor-pointer",children:"Copy Goose CLI Command"}),(0,h.jsx)("div",{className:"absolute bottom-full mb-2 left-1/2 -translate-x-1/2 hidden group-hover:block bg-zinc-800 text-white text-xs px-2 py-1 rounded shadow-lg whitespace-nowrap z-50",children:"Copies the CLI command to run this recipe"})]})]})]})]})}),v&&(0,h.jsx)("div",{className:"fixed inset-0 bg-black bg-opacity-60 z-50 flex items-center justify-center",children:(0,h.jsxs)("div",{className:"bg-white dark:bg-zinc-800 p-6 rounded-lg w-full max-w-md",children:[(0,h.jsx)("h3",{className:"text-lg font-semibold mb-4 text-zinc-900 dark:text-white",children:"Fill in parameters"}),_.map((e=>(0,h.jsxs)("div",{className:"mb-3",children:[(0,h.jsxs)("label",{className:"block text-sm text-zinc-700 dark:text-zinc-200 mb-1",children:[e.key," ","optional"===e.requirement&&(0,h.jsx)("span",{className:"text-zinc-400",children:"(optional)"})]}),(0,h.jsx)("input",{type:"text",value:x[e.key]||"",onChange:t=>k((n=>({...n,[e.key]:t.target.value}))),className:"w-full px-3 py-2 border border-zinc-300 dark:border-zinc-600 rounded bg-white dark:bg-zinc-700 text-zinc-900 dark:text-white"})]},e.key))),(0,h.jsxs)("div",{className:"flex justify-end gap-3",children:[(0,h.jsx)("button",{onClick:()=>w(!1),className:"text-sm text-zinc-600 dark:text-zinc-300 hover:underline",children:"Cancel"}),(0,h.jsx)("button",{onClick:()=>{const e=Object.entries(x).filter((e=>{let[,t]=e;return""!==t})).map((e=>{let[t,n]=e;return`${t}=${n}`})).join(" "),n=`goose run --recipe ${t?.localPath}${e?` --params ${e}`:""}`;navigator.clipboard.writeText(n),p.Ay.success("CLI command copied with params!"),w(!1)},className:"bg-purple-600 text-white px-4 py-2 rounded text-sm hover:bg-purple-700",children:"Copy Command"})]})]})})]})}},54631:(e,t,n)=>{"use strict";n.d(t,{d:()=>r,q:()=>a});const i=n(75878);function r(e){return s().find((t=>t.id===e))||null}async function a(e){const t=s();return e?t.filter((t=>t.title?.toLowerCase().includes(e.toLowerCase())||t.description?.toLowerCase().includes(e.toLowerCase())||t.action?.toLowerCase().includes(e.toLowerCase())||t.activities?.some((t=>t.toLowerCase().includes(e.toLowerCase()))))):t}function s(){return i.keys().map((e=>function(e){const t={id:e.id||e.title?.toLowerCase().replace(/\s+/g,"-")||"untitled-recipe",title:e.title||"Untitled Recipe",description:e.description||"No description provided.",instructions:e.instructions,prompt:e.prompt,extensions:Array.isArray(e.extensions)?e.extensions.map((e=>"string"==typeof e?{type:"builtin",name:e}:e)):[],activities:Array.isArray(e.activities)?e.activities:[],version:e.version||"1.0.0",author:"string"==typeof e.author?{contact:e.author}:e.author||void 0,action:e.action||void 0,persona:e.persona||void 0,tags:e.tags||[],recipeUrl:"",localPath:`documentation/src/pages/recipes/data/recipes/${e.id}.yaml`};if(Array.isArray(e.parameters)){for(const t of e.parameters)"required"!==t.requirement||t.value||(t.value=`{{${t.key}}}`);t.parameters=e.parameters}const n={title:t.title,description:t.description,instructions:t.instructions,prompt:t.prompt,activities:t.activities,extensions:t.extensions,parameters:t.parameters||[]},i=function(e){if("undefined"!=typeof window&&window.btoa)return window.btoa(unescape(encodeURIComponent(e)));return Buffer.from(e).toString("base64")}(JSON.stringify(n));return t.recipeUrl=`goose://recipe?config=${i}`,t}({...i(e).default||i(e),id:e.replace(/^.*[\\/]/,"").replace(/\.(yaml|yml)$/,"")})))}},88485:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Analyze java monorepo build failure",description:"Analyze java monorepo build failure",instructions:"follow the prompts to analyze java monorepo build failure",activities:["Fetch failed build parts","Download and parse logs","Identify root causes","Analyze test failures","Summarize common issues and next steps","Generate HTML report"],prompt:'Guidelines:\n- Use curl instead of browser-based extensions like fetch.\n- Use jq for JSON parsing and transformations.\n- Prefer creating reusable shell functions for repeated steps.\n- **Do NOT** make up any information, only use the information provided in the logs.\n\nStep 1:  Fetch Failed Build Parts\n    - run the command \n      ```\n      curl https://kochiku.sqprod.co/squareup/java/builds/{{build_id}}?format=json \\\n      | jq \'[.build.build_parts[] | select(.status == "failed") \\\n      | {id, build_id, kind, attempt_count, status}]\' > failed_builds.json\n      ```\n\nStep 2:\nFor each failed build part, create a loop to handle each part:\n2.1 Fetch the last attempt\n  - find the last attempt via curl https://kochiku.sqprod.co/squareup/java/builds/{build_id}/parts/{id}?format=json | jq \'.build_part.build_attempts[-1]\'\n2.2 fetch the stdout.log.gz and junit-report.html (if present)\n  - get artifact_id of the log files including stdout.log.gz and junit-report.html (if present) in the last attempt\n    Example of json\n      {\n      "build_part": {\n        "id": 841284189,\n        "build_id": 9937415,\n        "kind": "deployable-loan-gateway",\n        "paths": [\n          "loan-gateway:test"\n        ],\n        "status": "failed",\n        "elapsed_time": 812,\n        "build_attempts": [\n          {\n            "id": 612181475,\n            "build_part_id": 841284189,\n            "files": [\n              {\n                "build_artifact": {\n                  "id": 2125669387,\n                  "build_attempt_id": 612181475,\n                  "created_at": "2025-05-02T15:01:19.000-07:00",\n                  "updated_at": "2025-05-02T15:01:19.000-07:00",\n                  "log_file": {\n                    "url": "/build_artifacts/2125669387",\n                    "name": "squareup/java/build_9937415/part_841284189/attempt_612181475/stdout.log.gz"\n                  }\n                }\n              }\n            ]\n          }\n        ]\n      }\n    }\n  - construct the artifact_url https://kochiku.sqprod.co/build_artifacts/{artifact_id}\n  - add the part_url, stdout.log artifact_url and junit_report url in the failed_builds.json in the matching build_part block. If the file does not exist, set the field to null\n2.3 Analyze the stdout.log.gz file\n  - download stdout.log.gz file using curl command with follow redirect option\n  - find the error in this file error keywords of "error,failed,fails,fail,fatal"(case insensitive), and also include the 2 lines before and 10 lines after the found error, saved the result into a file with filtered_error_{artifact_id}.txt\n  - add the filtered_error_{artifact_id}.txt file name to the corresponding build part in failed_builds.json\n2.4 Root Cause Analysis for Filtered Error\n  - review filtered_error_{artifact_id}.txt\n  - Determine the root cause of the error\n  - Provide\n      1. Root cause\n        - Provide a comprehensive explanation of why the failure occurred.\n        - Explain the specific failure context (e.g., what phase failed \u2014 dependency resolution, test execution, compilation, etc.).  \n        - Mention any recent changes (code, dependencies, infra) if visible or likely from logs.\n      2. Detailed Justifications:\n        - Quote the relevant error messages or stack traces verbatim from the logs.\n        - Describe what each error message means, what tool/component is involved (e.g., Maven, Gradle, JUnit, Kotlin compiler, etc.), and how it relates to the build.\n        - Provide background context \u2014 e.g., "This type of error is common when ..." or "Historically, this happens when ..."\n        - Discuss any patterns observed across multiple failed parts with similar symptoms.\n      3. Suggestions for Fixing the Problem:\n        - Propose practical and actionable fixes.\n        - Suggest specific files or lines that may need changes (e.g., build.gradle, test class). \n        - If applicable, recommend mitigations (e.g., retry the build, invalidate caches, contact a service owner).\n        - For flaky tests or environmental issues, suggest diagnostics (e.g., re-run in isolation, check recent infra changes).\n    - Add this analysis (root cause, justification, and suggestions) to the relevant build part in failed_builds.json.\n2.5 Check for Test Failures\n  - If the stdout.log.gz file contains "tests_result=3", it indicates some tests failed in this build\n  - download junit-report.html use curl command (if available) with follow redirect option\n  - Parse the HTML to extract:\n    - Names of failed test classes and methods.\n    - Associated error messages or stack traces.\n  - save this information to filtered_test_error_{artifact_id}.html\n2.6 Root Cause Analysis for Test Failures\n  - review filtered_test_error_{artifact_id}.html\n  - Determine the root cause of the test failures\n  - Provide\n      1. Root cause\n        - Explain why the test failed, including what condition, assertion, or runtime behavior caused it.\n        - Identify whether the issue is likely caused by a code regression, incorrect test setup, flaky behavior, environment issues, or external dependencies (e.g., services, databases).\n        - Mention if multiple test failures appear to share a root cause (e.g., shared fixture or mock failure).\n      2. Detailed Justifications:\n        - Quote the exact test name (class and method), along with failure messages or stack traces.\n        - Explain what the test was trying to validate, and what part of the system it targets (e.g., business logic, edge case, error handling). \n        - Provide background: has this test failed before? Is it marked flaky or is the logic fragile?\n        - If applicable, explain why this issue might only occur in CI (e.g., timing, parallelism, test order).\n      3. Suggestions for Fixing the Problem\n        - Suggested for fixing the problem\n  - add the root cause analysis, detailed justifications and potential fix suggestions to the matching build part in the `failed_builds.json`\n\nStep 3: Verify Completeness\n  - Verify each build part in failed_builds.json have been analyzed.\n  - If any of the build part has not been verified, repeat Step 2 for these build part\n\nStep 4:\n  - Review all build part analyses.\n  - Identify recurring root causes and classify them as common issues.\n  - Suggest immediate next steps that apply across multiple build parts (e.g., restarting a flaky test, checking dependency versions).\n  - Add a common_issues field and an immediate_next_steps field to the failed_builds.json file.\nStep 5: \n  - Using the completed failed_builds.json, create a readable HTML report named: java_monorepo_build_analysis_{build_id}.html\n  - The HTML report should include:\n    - A list of issues (common issues are grouped together)\n      For each issue, list:\n        - The associated build parts and part urls\n        - Links to stdout.log.gz and junit-report.html.\n        - Root cause\n        - Justifications\n        - Fix suggestions\n    - Immediate next steps for fixing the issues.\n',extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],parameters:[{key:"build_id",input_type:"number",requirement:"user_prompt",description:"the failed build id to analyze"}],author:{contact:"lifeizhou-ap"}}},46611:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={id:"analyze-pr",version:"1.0.0",title:"Analyse PR",author:{contact:"douwe"},description:"Analyse a pr",instructions:"Your job is to analyse and explain a PR",activities:["Query authentication logs","Investigate Sentry reports","Correlate device usage with auth events","Query Snowflake user identity tables","Review repo code for auth issues"],parameters:[{key:"pr",input_type:"string",requirement:"required",description:"name of the pull request"},{key:"repo",input_type:"string",requirement:"optional",description:"name of the repo. uses the current one if not selected",default:""}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0},{type:"builtin",name:"memory",display_name:"Memory",timeout:300,bundled:!0,description:"For storing and retrieving formating preferences that might be present"}],prompt:"Analyze the pr with the name {{ pr }}. Find out what has changed, try to figure out why these\nchanges were made and tell the user in detail what you found out.\n{% if repo %}\nWe are working with the {{ repo }} repository, so make sure to add that to all commands.\n{% endif %}\n\nSteps:\n1. Find the actual pull request. {{ pr }} is the name or part of it. You can just run\n   `gh pr list`\n   and see which prs are open. Note which one the user is talking about\n2. Look at what is changed. You can run:\n   `gh pr view <pr-number> --comments --commits --files`\n   to get an overview.\n3. Optionally: if this looks complicated you could check out the relevant commit and have\n   a look at the files involved to get more context. If you do this, mark which branch you\n   were on. If there are pending changes, do a git stash\n4. Gather your thoughts and tell the user what changed, which changes look like they might\n   be worth an extra look and give them an idea of maybe why these changes were needed\n5. Clean up after yourself. If you cloned a repository or checked out a commit, make sure\n   you return the state to what it was before. So if in step 3 you changed branch, change\n   it back. If you had git stashed something, stash pop it again.\n"}},32667:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Blokker Snapshot Migrator",author:{contact:"jadam"},description:"Migrate to Snapshot Testing for FormBlockers",instructions:"Follow the prompts to migrate a service to snapshot testing",activities:["Create a migration branch","Add plasma-testing dependency to Gradle config","Identify usages of FormBlocker.Builder()","Locate and update associated test classes","Commit test and snapshot changes","Create and submit a pull request"],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:600,bundled:!0}],prompt:'# Pre-Instructions\n\n## Working Directory\n- No files outside the directory {{working_directory}} should be modified.\n- No commands should be run outside of the directory {{working_directory}}.\n\n## Gradle\n- To run gradle, use `bin/gradle <project> <task>` from the project root. There is NO `gradlew` script.\n- If you are working in cash-server, the repository consists of many projects. You should usually specify a specific project with `-p` any time you are calling gradle from the root directory. The gradle to use is located at `cash-server/bin/gradle` even if you\'re in a subproject.\n\n# Instructions\nPerform the following steps to migrate to snapshot testing:\n\n1. Create a new branch and check it out for the migration using the following command:\n\n```bash\ngit checkout -b $USER/$(date +%m%d%Y)-formblocker-snapshot-migration\n````\n\n2. Ensure that the `plasma-testing` library is added to the `gradle/libs.versions.toml` file. If it is not, add it:\n\n```toml\n[libraries]\nplasmaTesting = { module = "com.squareup.plasma:plasma-testing", version = "2025.04.01-1743540086-eeb2f79" }\n```\n\n3. Find all files that FormBlocker.Builder() is used in.\n\n4. Locate the associated test classes. These are the files to update.\n\n5. Update the `build.gradle.kts` files of the modules that contain the test classes to include the following dependencies:\n\nIf `cash-server` is not in the current path:\n```kotlin\ndependencies {\n  testImplementation(libs.plasmaTesting)\n}\n```\n\nIf `cash-server` is in the current path:\n```kotlin\ndependencies {\n  testImplementation(project(":plasma:plasma-testing"))\n}\n```\n\n6. Update any test methods from these classes that test the outputs of a FormBlocker.Builder() to use the BlockerTester.snapshot() method.\n\nMake sure that any modified test files have the import `import com.squareup.cash.blockertesting.BlockerTester`.\n\nExamples:\n```kotlin\nimport com.squareup.cash.blockertesting.BlockerTester\n\nclass SomeBlockerTest {\n  @Inject private lateinit var requirementHandler: SomeRequirementHandler\n\n  @Test\n  fun testManual() {\n    // Test some blocker you built by hand.\n    // This isn\'t useful in practice, but it\'s simple and illustrative\n    val blocker = FormBlocker.Builder().elements(listOf(text("Sample"))).build()\n    BlockerTester.snapshot("SNAPSHOT_NAME", blocker)\n  }\n\n  @Test\n  fun testHandler() {\n    // Test a blocker you get back from calling a Plasma FlowHandler or RequirementHandler\n    val response = requirementHandler.plan(request)\n    val bytes = response.plan.next_step.ui_form.blocker\n    val blocker = BlockerDescriptor.ADAPTER.decode(bytes)\n    BlockerTester.snapshot("REQUIREMENT_SNAPSHOT_NAME", blocker)\n  }\n\n  @Test\n  fun testHandlerBetter() {\n    // Test a blocker you get back from calling a Plasma FlowHandler or RequirementHandler,\n    // using Plasma\'s test helpers to simplify things\n    val blocker = requirementHandler.testPlan(request).blockerDescriptor\n    BlockerTester.snapshot("REQUIREMENT_SNAPSHOT_NAME", blocker)\n  }\n\n  @Test\n  fun testHandlerIgnoreId() {\n    // Test a blocker you get back from calling a Plasma FlowHandler or RequirementHandler,\n    // using Plasma\'s test helpers to simplify things and ignoring all "id" fields.\n    val blocker = requirementHandler.testPlan(request).blockerDescriptor\n    BlockerTester.snapshot("REQUIREMENT_SNAPSHOT_NAME", blocker, ignoredFields = listOf("**.id"))\n  }\n\n  @Test\n  fun testMatcher() {\n    // Test some blocker you built by hand using the matcher.\n    val blocker = FormBlocker.Builder().elements(listOf(text("Sample"))).build()\n    // This calls BlockerTester.snapshot()\n    blocker shouldMatchSnapshot "SNAPSHOT_NAME"\n  }\n}\n```\n\nSnapshot names must be unique so give a name that is descriptive of the test and blocker being tested.\nSnapshot names must be valid file names, and thus cannot contain forward slashes.\n\nExample Failure Output:\n```\nFailure: REQUIREMENT_SNAPSHOT_NAME blocker snapshot does not match the existing REQUIREMENT_SNAPSHOT_NAME.json\nrun `gradle {module}:updateSnapshots` to generate and overwrite the stale snapshots.\nelements[0].text_element.text\n  Expected: MY_EXPECTED_TEXT\n  got: SOME_OTHER_UNEXPECTED_TEXT\n```\n\n7. You may need to run a `spotlessKotlin` or `spotlessApply` gradle task if it\'s a plugin in the repository. This will reformat the file to match the style guide.\n\nThis will need to be run for any submodules that have been updated. For example if you have changed files in a module called `flows`, you need to run:\n```bash\nbin/gradle :flows:spotlessApply\n```\nOR\n```bash\nbin/gradle :flows:spotlessKotlin\n```\n\nIf `cash-server` is in the path, you can run the following command from the root of the `cash-server` repository:\n```bash\nbin/gradle :<service-name>:<submodule-name>:spotlessApply\n```\n\nIf the submodule you are updating doesn\'t have a `spotless` gradle task, then ignore this step.\n\n8. Run the tests for each changed file after updating it so that the snapshot files are created.\n\n9. If the test fails for any reason, review the error, make any fixes, and re-run the test method until it passes.\n\n10. If the test passes, commit the changes with a commit message summarizing the change. Include the generated snapshot `.json` files in the commit. They will be located within a resources folder in the same sub-module as the test files changed.\n\n11. You should only be committing updates to test files, build.gradle.kts files, libs.versions.toml and the generated snapshot files.\n\n# Post-Instructions\n- Push your changes to the remote repository\n- Create a pull request with the following command:\n\n```bash\ngh pr create --assignee "@me" --body "This PR migrates tests that inspect the output of `FormBlocker.Builder()` to use [snapshot testing](https://cash-dev-guide.sqprod.co/product_velocity/plasma/test/tools/?h=snapshot#snapshot-testing) instead. This is a more robust way to test the output of a blocker, and it will make it easier to refactor blockers in the future." --title "Migrate to snapshot testing for FormBlocker.Builder() usages"\n```\n',parameters:[{key:"working_directory",input_type:"string",requirement:"user_prompt",description:"The working directory of the service to migrate"}]}},6013:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Generate Change Logs from Git Commits",description:"Generate Change Logs from Git Commits",instructions:"Follow the prompts to generate change logs from the provided git commits",activities:["Retrieve and analyze commits","Categorize changes","Format changelog entries","Update CHANGELOG.md"],prompt:'Task: Add change logs from Git Commits\n1. Please retrieve all commits between SHA {{start_sha}} and SHA {{end_sha}} (inclusive) from the repository.\n\n2. For each commit:\n  - Extract the commit message\n  - Extract the commit date\n  - Extract any referenced issue/ticket numbers (patterns like #123, JIRA-456)\n\n3. Organize the commits into the following categories:\n  - Features: New functionality added (commits that mention "feat", "feature", "add", etc.)\n  - Bug Fixes: Issues that were resolved (commits with "fix", "bug", "resolve", etc.)\n  - Performance Improvements: Optimizations (commits with "perf", "optimize", "performance", etc.)\n  - Documentation: Documentation changes (commits with "doc", "readme", etc.)\n  - Refactoring: Code restructuring (commits with "refactor", "clean", etc.)\n  - Other: Anything that doesn\'t fit above categories\n\n4. Format the release notes as follows:\n  \n  # [Version/Date]\n  \n  ## Features\n  - [Feature description] - [PR #number](PR link)\n  \n  \n  ## Bug Fixes\n  - [Bug fix description] - [PR #number](PR link)\n  \n  [Continue with other categories...]\n  \n  Example:\n  - Implement summary and describe-commands for better sq integration - [PR #369](https://github.com/squareup/dx-ai-toolbox/pull/369)\n  \n5. Ensure all the commit items has a PR link. If you cannot find it, try again. If you still cannot find it, use the commit sha link instead. For example: [commit sha](commit url)\n\n6. If commit messages follow conventional commit format (type(scope): message), use the type to categorize and include the scope in the notes.\n\n7. Ignore merge commits and automated commits (like those from CI systems) unless they contain significant information.\n\n8. For each category, sort entries by date (newest first).\n\n9. formatted change logs as a markdown document\n\n10. Create an empty CHANGELOG.md file if it does not exist\n\n11. Read CHANGELOG.md and understand its format.\n\n11. Insert the formatted change logs at the beginning of the CHANGELOG.md, and adjust its format to match the existing CHANGELOG.md format. Do not change any existing CHANGELOG.md content.\n',extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],parameters:[{key:"start_sha",input_type:"string",requirement:"user_prompt",description:"the start sha of the git commits"},{key:"end_sha",input_type:"string",requirement:"user_prompt",description:"the end sha of the git commits"}],author:{contact:"lifeizhou-ap"}}},14679:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={id:"clean-up-feature-flag",title:"Clean Up Feature Flag",description:"Automatically clean up all references of a fully rolled out feature flag from a codebase and make the new behavior the default.",instructions:"Your job is to systematically remove a fully rolled out feature flag and ensure the new behavior is now the default. Use code search tools like ripgrep to identify all references to the flag, clean up definition files, usage sites, tests, and configuration files. Then create a commit and push changes with clear commit messages documenting the flag removal.\n",author:{contact:"amitdev"},extensions:[{type:"builtin",name:"developer"}],activities:["Remove feature flag definitions","Clean up feature flag usage sites","Update affected tests","Remove flag configurations","Document flag removal"],parameters:[{key:"feature_flag_key",input_type:"string",requirement:"required",description:"Key of the feature flag",value:"MY_FLAG"},{key:"repo_dir",input_type:"string",requirement:"optional",default:"./",description:"Directory of the codebase",value:"./"}],prompt:"Task: Remove a feature flag that has been fully rolled out, where the feature flag's functionality should become the default behavior.\n\nContext:\n\nFeature flag key: {{feature_flag_key}}\nProject: {{repo_dir}}\nFeature is fully rolled out and stable, meaning the feature flag is always evaluated to true or Treatment, etc.\n\nSteps to follow:\n\n1. Check out a *new* branch from main or master named using the feature flag key.\n2. Find the feature flag constant/object that wraps the key.\n3. Search for all references to the constant/object using ripgrep or equivalent tools.\n4. For each file that contains references:\n   - **Definition files**: Remove the flag definition and related imports.\n   - **Usage sites**: Remove conditional logic and default to the new behavior. Clean up related imports.\n   - **Test files**: Remove tests that cover the 'disabled' state of the flag and update remaining ones. Clean up mocks and imports.\n   - **Configuration files**: Remove entries related to the feature flag.\n5. Re-run a full-text search to ensure all references (and imports) are removed.\n6. Clean up now-unused variables or functions introduced solely for the flag.\n7. Double-check for and remove any leftover imports or dead code.\n8. Create a commit with **only the files affected by this cleanup** (don\u2019t use `git add .`).\n9. Push the branch to origin.\n10. Open a GitHub PR using: `https://github.com/squareup/<repo-name>/compare/<branch-name>` and replace the repo and branch placeholders.\n\nUse clear commit messages like:\n\n  chore(flag-cleanup): remove <feature_flag_key> flag from codebase\n\nExplain the flag was fully rolled out and the new behavior is now default.\n"}},22599:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Create Kafka Topic",author:{contact:"danielst-block"},description:"Create a new Kafka topic with specified parameters.",activities:["Check for existing topic name conflicts","Validate publisher and subscriber names","Calculate optimal partition count","Generate Kafka topic configuration","Create topic directory and config files"],parameters:[{key:"topic_name",input_type:"string",requirement:"required",description:"The name of the Kafka topic to create"},{key:"owner",input_type:"string",requirement:"required",description:"The name/identifier of owner."},{key:"publisher",input_type:"string",requirement:"required",description:"The name/identifier of the publisher service or application"},{key:"subscribers",input_type:"string",requirement:"required",description:'Comma-separated list of subscriber services or applications that will consume from this topic (e.g., "service1,service2,service3")'},{key:"throughput",input_type:"string",requirement:"optional",description:"Expected throughput. Used to calculate optimal number of partitions for the topic",default:"unknown"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],instructions:"You are a Kafka topic creation assistant. Your job is to help create a new Kafka topic HCL \ndefinitions with the specified configuration including topic name, publisher, owner, \nsubscribers, and optional throughput. Follow the existing folder structure and conventions.\n",prompt:"1. Create a {{ topic_name }} directory for a Kafka topic based on the following parameters:\n  - Topic name: {{ topic_name }}\n  - Owner: {{ owner }}\n  - Publisher: {{ publisher }}\n  - Subscribers: {{ subscribers }}\n  - Throughput: {{ throughput }} messages/second (if provided)\n2. Ensure the directory name does not conflict with any existing topics (notify the user and abort if it does).\n3. Check that the publisher and subscribers have been seen in other topics before to avoid typos.\n4. If throughput is provided - calculate the optimal number of partitions. Otherwise, default to 4 partitions.\n5. Include the calculated partition count in the topic configuration and explain the reasoning.\n"}},1807:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"dev guide migration from a specific file or files in a directory",description:"dev guide migration from a specific file or files in a directory",instructions:"Follow the prompts to migrate the doc page from source file(s) to target folder.",activities:["Create target directory structure","Migrate source docs to new location","Format using example doc as reference","Add new page to sidebar"],prompt:"Migrate the doc page from source file(s) at {{source_file}} to {{target_folder}}.  Please follow the instructions below:  \n1. Create the parent directory if the parent directory of the target file does not exist\n2. use {{example_file}} as a reference for the doc format\n3. retain all the information of the source file(s) in the target file \n4. If the page is not in the sidebar, add it in {{sidebar_file}}\n5. Ensure the target files \n      - has preserved the original content\n      - has correct formatting\n      - has clear and well-organized file structure\n",extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],parameters:[{key:"source_file",input_type:"file",requirement:"user_prompt",description:"the source file(s) or the folder to migrate"},{key:"target_folder",input_type:"file",requirement:"user_prompt",description:"the target folder to migrate"},{key:"example_file",input_type:"file",requirement:"user_prompt",description:"the example file to follow the doc format"},{key:"sidebar_file",input_type:"file",requirement:"user_prompt",description:"the sidebar file to add the new doc page"}],author:{contact:"lifeizhou-ap"}}},11315:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"0.1.0",title:"Fix PR CI failures",description:"Iteratively fix errors detected by Kochiku CI on the current PR",instructions:"follow the prompts to fix CI failures on your PR",activities:["Fetch CI build status","Analyze Kochiku build errors","Split issues into individual tasks","Fix and verify each issue iteratively","Mark resolved issues"],prompt:'Guidelines:\n- Use the commands provided to identify and fix CI failures in your PR\n- Apply smallest-possible fixes and run scoped tests after each change\n- Stop if the build is not in a failed state\n- Process one issue at a time, fixing and testing before moving to the next\n- Do not ask the user any questions, do your best to fix autonomously.\n\nStep 1: Find the CI build URL for your current commit\n\nRun this command to get the Kochiku build URL for your current commit:\n```\ngh pr view $(gh pr list --search $(git rev-parse HEAD) --json number -q ".[0].number") --json statusCheckRollup -q ".statusCheckRollup[] | select(.context == \\"Kochiku\\") | .targetUrl"\n```\n\nStep 2: Check if the build has failed\n\nExtract the build ID from the URL (the number at the end) and check its state:\n```\nsq curl -s -L -H "Content-type: application/json" -H "Accept: application/json" "https://kochiku.sqprod.co/builds/BUILD_ID" | jq -r \'.build.state\'\n```\n\nIf the output is not "failed", then stop - there\'s nothing to fix yet.\n\nStep 3: Get your repository name\n\n```\nbasename -s .git $(git remote get-url origin)\n```\n\nStep 4: Fetch the analysis of what failed\n\nReplace REPO and BUILD_ID with the values from steps 2 and 3:\n```\nsq curl -s -X POST \\\n   -H "Content-Type: application/json" \\\n   -d \'{"repository":"REPO", "build_id": "BUILD_ID", "ci_type": "KOCHIKU"}\' \\\n  "https://ci-results.sqprod.co/services/squareup.ciresults.service.CiResultsService/GetBuildMetadataWithAnalysis" | jq -r \'.issues\' > issues.json\n```\n\nStep 5: Split the issues into individual files\n\nThis command will take the issues.json file and create a separate file for each issue:\n```\njq -c \'.[] | @base64\' issues.json | while read issue; do\n  decoded=$(echo $issue | base64 --decode)\n  index=$((index+1))\n  echo $decoded > "issue_${index}.todo.json"\n  echo "Created issue_${index}.todo.json"\ndone\n```\n\nStep 6: Process each issue\n\nFor each issue file (issue_1.todo.json, issue_2.todo.json, etc.):\n\nExamine the issue file to understand the issue. Attempt to fix the issue\nand verify the fix by running the appropriate commands (you\'ll have to infer these).\n\nOnce you have fixed & verified the issue, rename the file from `issue_N.todo.json` to `issue_N.resolved.json`\nand move on to the next issue.\n\nKeep iterating until there are no more `issue_N.todo.json` files.\n',extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:600,bundled:!0}],author:{contact:"tmellor-block"}}},49174:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Migrate Cypress tests to Playwright",author:{contact:"joahg"},description:"Migrate Cypress tests to Playwright",instructions:"Your job is to migrate cypress tests to playwright tests.",activities:["Analyze Cypress test file","Convert Cypress syntax to Playwright","Migrate custom commands and helpers","Update imports and async handling","Save Playwright test in target directory"],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"You are tasked with migrating a Cypress test to Playwright. \n\nCypress test file: {{ cypress_test_file }}\nTarget directory: {{ target_directory }}\n\nPlease follow these steps:\n\n1. **Analyze the Cypress test file**: Examine the Cypress test file at {{ cypress_test_file }}, including its structure, commands, and any custom helper functions used.\n\n2. **Migrate the test structure**: Convert Cypress test syntax to Playwright:\n   - Replace `describe()` and `it()` with Playwright's `test.describe()` and `test()`\n   - Convert `cy.visit()` to `page.goto()`\n   - Convert `cy.get()` to appropriate Playwright locators\n   - Convert assertions from Cypress format to Playwright's `expect()` assertions\n   - Handle async/await patterns properly in Playwright\n\n3. **Migrate Cypress commands**: Convert common Cypress commands to Playwright equivalents:\n   - `cy.click()` \u2192 `locator.click()`\n   - `cy.type()` \u2192 `locator.fill()` or `locator.type()`\n   - `cy.should()` \u2192 `expect(locator).to**()`\n   - `cy.wait()` \u2192 `page.waitForTimeout()` or better, specific wait conditions\n   - `cy.intercept()` \u2192 `page.route()`\n\n4. **Migrate helper functions**: If the Cypress test uses custom commands or helper functions:\n   - Convert Cypress custom commands to Playwright helper functions\n   - Ensure helper functions are properly imported and available in the target directory\n   - Update function signatures to work with Playwright's page object\n\n5. **Update imports and setup**: \n   - Add proper Playwright imports (`import { test, expect } from '@playwright/test'`)\n   - Remove Cypress-specific imports\n   - Ensure proper test configuration and setup\n\n6. **Handle test data and fixtures**: Convert any Cypress fixtures or test data to work with Playwright\n\nCreate the migrated Playwright test in the target directory, maintaining the same test coverage and functionality as the original Cypress test. Use the same base filename but with appropriate Playwright test naming conventions (e.g., .spec.ts or .test.ts).\n",parameters:[{key:"cypress_test_file",input_type:"file",requirement:"user_prompt",description:"The specific Cypress test file to migrate (e.g., cypress/e2e/login.cy.js)"},{key:"target_directory",input_type:"file",requirement:"user_prompt",description:"The target directory where the Playwright test should be created"}]}},78984:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"migrate from poetry to uv",description:"migrate from poetry to uv",instructions:"Follow the instructions to move the project from using `poetry` to `uv`",author:{contact:"jamadeo"},activities:["Check if project already uses uv","Run migration using uvx","Remove poetry-related files and virtualenv","Run uv sync"],prompt:"The current project uses `poetry` for Python environment and dependency management. We want to use `uv` instead.\n\nFirst, verify that the above is true. If the project is actually already using `uv`, you can stop.\n\nStart by running `uvx migrate-to-uv`. If you don't have `uv` installed, use `hermit install uv` to add it. If hermit isn't set up, use `hermit init` to do so.\n\nOnce `migrate-to-uv` has run, delete any local virtualenvs (often located at ./.venv) and run `uv sync`.\n\nGrep for other uses of `poetry` in the project. If you can switch these commands to `uv`, do so. If not, just make a note of it.\n",extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}]}},3799:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"PR Demo Planner",author:{contact:"lifei"},description:"Transforms technical Pull Requests into effective demonstrations that showcase functionality and value",activities:["Analyze PR changes for demonstrable improvements","Create demo script and narrative flow","Build visual storyboard with before/after comparisons","Suggest environments and test data for effective demo","Translate technical changes into business value"],instructions:"You are a PR Demo Planner, an assistant specialized in transforming technical Pull Requests into engaging demonstrations.\n\nYour capabilities include:\n1. Analyzing PR changes to identify demonstrable features and improvements\n2. Creating structured demo scripts based on code changes\n3. Generating visual storyboards for demonstrations\n4. Helping prepare before/after comparisons that highlight improvements\n5. Crafting narratives that connect technical changes to business value\n6. Suggesting demo environments and test data\n\nWhen helping developers convert PRs to demos:\n\n- First understand the PR's purpose, scope, and technical changes\n- Identify the most visually demonstrable aspects of the changes\n- Create a narrative flow that showcases the improvements\n- Focus on before/after comparisons when applicable\n- Prepare for both technical and non-technical audiences\n- Include setup instructions to ensure smooth demonstrations\n- Suggest ways to highlight performance improvements or bug fixes\n\nYou have access to reference materials:\n- {{ recipe_dir }}/demo-formats.md for different demonstration approaches\n- {{ recipe_dir }}/demo-script-templates.md for structured presentation formats\n- {{ recipe_dir }}/technical-to-visual-guide.md for translating code changes to visual demonstrations\n\nAlways aim to create demonstrations that clearly show the value of the changes made in the PR.\n",extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"I need help converting my Pull Request into an effective demonstration. Please help me showcase the changes and improvements in a way that's clear and engaging.\n\nYou can assist me with:\n- Analyzing my PR to identify demonstrable features\n- Creating a structured demo script\n- Generating a visual storyboard\n- Preparing before/after comparisons\n- Crafting a narrative that explains the value\n- Setting up an effective demo environment\n\nThis is my PR: {{ pr_url }}\n",parameters:[{key:"pr_url",input_type:"string",requirement:"required",description:"The URL of the PR to convert into a demo."}]}},59313:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={id:"pr-generator",title:"PR Generator",description:"Automatically generate pull request descriptions based on changes in a local git repo.",instructions:"Your job is to generate descriptive and helpful pull request descriptions without asking for additional information. Generate commit messages and branch names based on the actual code changes.\n",author:{contact:"lifeizhou-ap"},extensions:[{type:"builtin",name:"developer"},{type:"builtin",name:"memory"}],parameters:[{key:"git_repo_path",input_type:"string",requirement:"required",description:"Path to the local git repository",value:"{{git_repo_path}}"},{key:"push_pr",input_type:"boolean",requirement:"optional",description:"Whether to push changes and create a PR",value:!1}],activities:["Generate PR","Analyze staged git changes","Create PR description"],action:"Generate PR",prompt:"Analyze the staged changes and any unpushed commits in the git repository {{git_repo_path}} to generate a comprehensive pull request description. Work autonomously without requesting additional information.\n\nAnalysis steps:\n1. Get current branch name using `git branch --show-current`\n2. If not on main/master/develop:\n   - Check for unpushed commits: `git log @{u}..HEAD` (if upstream exists)\n   - Include these commits in the analysis\n3. Check staged changes: `git diff --staged`\n4. Save the staged changes diff for the PR description\n5. Determine the type of change (feature, fix, enhancement, etc.) from the code\n\nGenerate the PR description with:\n1. A clear summary of the changes, including:\n   - New staged changes\n   - Any unpushed commits (if on a feature branch)\n2. Technical implementation details based on both the diff and unpushed commits\n3. List of modified files and their purpose\n4. Impact analysis (what areas of the codebase are affected)\n5. Testing approach and considerations\n6. Any migration steps or breaking changes\n7. Related issues or dependencies\n\nUse git commands:\n- `git diff --staged` for staged changes\n- `git log @{u}..HEAD` for unpushed commits\n- `git branch --show-current` for current branch\n- `git status` for staged files\n- `git show` for specific commit details\n- `git rev-parse --abbrev-ref --symbolic-full-name @{u}` to check if branch has upstream\n\nFormat the description in markdown with appropriate sections and code blocks where relevant.\n\n{% if push_pr %}\nExecute the following steps for pushing:\n1. Determine branch handling:\n   - If current branch is main/master/develop or unrelated:\n     - Generate branch name from staged changes (e.g., 'feature-add-user-auth')\n     - Create and switch to new branch: `git checkout -b [branch-name]`\n   - If current branch matches changes:\n     - Continue using current branch\n     - Note any unpushed commits\n\n2. Handle commits and push:\n   a. If staged changes exist:\n      - Create commit using generated message: `git commit -m \"[type]: [summary]\"`\n      - Message should be concise and descriptive of actual changes\n   b. Push changes:\n      - For existing branches: `git push origin HEAD`\n      - For new branches: `git push -u origin HEAD`\n\n3. Create PR:\n   - Use git/gh commands to create PR with generated description\n   - Set base branch appropriately\n   - Print PR URL after creation\n\nBranch naming convention:\n- Use kebab-case\n- Prefix with type: feature-, fix-, enhance-, refactor-\n- Keep names concise but descriptive\n- Base on actual code changes\n\nCommit message format:\n- Start with type: feat, fix, enhance, refactor\n- Followed by concise description\n- Based on actual code changes\n- No body text needed for straightforward changes\n\nDo not:\n- Ask for confirmation or additional input\n- Create placeholder content\n- Include TODO items\n- Add WIP markers\n{% endif %}\n"}},55536:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Readme Bot",author:{contact:"DOsinga"},description:"Generates or updates a readme",instructions:"You are a documentation expert",activities:["Scan project directory for documentation context","Generate a new README draft","Compare new draft with existing README.md"],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"Here's what to do step by step:\n  1. The current folder is a software project. Scan it and learn as\n     much as possible.\n  2. Based on what you find, write a read me file that contains a\n     general description of the project, how to get started and how\n     to run the tests. Only mention future plans if you find explicit\n     todo's. Do not write about future plans or licenses or anything\n     that you can't find explicit support for.\n  3. Write this out as README.tmp.md.\n  4. Look at the existing README.md. If it exists and the version you\n     wrote out is not really better, just tell the user that what\n     exists is really good enough and you can exit.\n  5. If your version is better or no README.md exists, make your version\n     the current one\n  6. If you are on main or master, create a new branch\n  7. If the only chance at this point is the modification to the the\n     README.md, create a new commit \n  8. Clean up after yourself, delete the README.tmp.md after use.\n"}},85636:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Recipe Generator",author:{contact:"iYung"},description:"Creates other recipes",parameters:[{key:"prompt",input_type:"string",requirement:"required",description:"Description of what I want the recipe to do. Could be a file path"}],prompt:"Recipes are a set of instructions.\n\nHere is what a recipe should look like:\n```yaml\nversion: 1.0.0\ntitle: Title of my recipe\ndescription: Recipe Template\nprompt: Write your prompt in here\nextensions:\n  - type: builtin\n    name: developer\n    display_name: Developer\n    timeout: 300\n    bundled: true\n#only required if recipe description asks for user input\n#parameters are used in within prompt like \\{\\{ key }} and must be present\nparameters:\n  - key: example_parameter\n    input_type: string or number\n    requirement: required or optional\n    description: Description of the paramater.\n```\n\nImportant notes:\n- title is the name of the recipe\n- description is a short summary of what the recipe does\n- parameters are used within prompt like \\{\\{ key }} and must be present if mentioned in the recipe description\n\nUnder prompt can you write instructions that achieve\n{{ prompt }}\n\nIf the above is a file path, read the file to determine the goal.\n",extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}]}},35225:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Python un-AI",author:{contact:"douwe"},description:"Remove typical AI artifacts from Python code",instructions:"Your job is to write a remove AI artifacts from Python code",activities:["Remove redundant comments","Fix exception handling","Modernize typing","Inline trivial functions"],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"Look at the file: {{ file_name }}\nApply the following fixes:\n1. Remove any comment that replicates the name of a function or describes the next statement\n   but does not add anything. Like if it says # call the server and it is followed by a\n   statement call_server(), that's pointless\n2. Any try.. except block where we catch bare Exception, remove that or if you can find a\n   specific exception to catch and it makes sense since we can actually do something better\n   catch that. But in general consider whether we need an exception like that, we don't want\n   to ignore errors and quite often the caller is in a better state to do the right thing\n   or even if it is a genuine error, the user can just take action\n3. Modernize the typing used (if any). Don't use List with a capital, just use list. Same for\n   Dict vs dict etc. Also remove Optional and replace with |None. Use | anywhere else where\n   it fits too.\n4. Inline trivial functions that are only called once, like reading text from a file.\n",parameters:[{key:"file_name",input_type:"file",requirement:"user_prompt",description:"the full path to the python file you want to sanitize"}]}},75878:(e,t,n)=>{var i={"./analyze-java-monorepo-failures.yaml":88485,"./analyze-pr.yaml":46611,"./blokker-snapshot-migrator.yaml":32667,"./change-log.yaml":6013,"./clean-up-feature-flag.yaml":14679,"./create-kafka-topic.yaml":22599,"./dev-guide-migration.yaml":1807,"./fix-pr-ci-failures.yaml":11315,"./migrate-cypress-test-to-playwright.yaml":49174,"./migrate-from-poetry-to-uv.yaml":78984,"./pr-demo-planner.yaml":3799,"./pull-request-generator.yaml":59313,"./readme-bot.yaml":55536,"./recipe-generator.yaml":85636,"./remove-ai-artifacts-from-python-code.yaml":35225};function r(e){var t=a(e);return n(t)}function a(e){if(!n.o(i,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return i[e]}r.keys=function(){return Object.keys(i)},r.resolve=a,e.exports=r,r.id=75878},75395:(e,t,n)=>{"use strict";n.d(t,{A:()=>o});var i=n(96540);const r=(...e)=>e.filter(((e,t,n)=>Boolean(e)&&""!==e.trim()&&n.indexOf(e)===t)).join(" ").trim();var a={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};const s=(0,i.forwardRef)((({color:e="currentColor",size:t=24,strokeWidth:n=2,absoluteStrokeWidth:s,className:o="",children:l,iconNode:c,...d},u)=>(0,i.createElement)("svg",{ref:u,...a,width:t,height:t,stroke:e,strokeWidth:s?24*Number(n)/Number(t):n,className:r("lucide",o),...d},[...c.map((([e,t])=>(0,i.createElement)(e,t))),...Array.isArray(l)?l:[l]]))),o=(e,t)=>{const n=(0,i.forwardRef)((({className:n,...a},o)=>{return(0,i.createElement)(s,{ref:o,iconNode:t,className:r(`lucide-${l=e,l.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,n),...a});var l}));return n.displayName=`${e}`,n}},16445:(e,t,n)=>{"use strict";n.d(t,{A:()=>i});const i=(0,n(75395).A)("ArrowLeft",[["path",{d:"m12 19-7-7 7-7",key:"1l729n"}],["path",{d:"M19 12H5",key:"x3x0zl"}]])}}]);