---
title: Local Models
hide_title: true
description: Use local models to enhance your Goose experience with tools like Ollama, llama.cpp, and more
---

import Card from '@site/src/components/Card';
import styles from '@site/src/components/Card/styles.module.css';

<h1 className={styles.pageTitle}>Local Models</h1>
<p className={styles.pageDescription}>
  Local models allow you to run and manage machine learning models directly on your device, providing enhanced privacy.
</p>

<div className={styles.categorySection}>
  <h2 className={styles.categoryTitle}>ðŸ“š Ollama Guides</h2>
  <div className={styles.cardGrid}>
    <Card 
      title="Ollama Setup"
      description="Using Ollama with Goose"
      link="/docs/guides/local-models/ollama-setup"
    />
    <Card 
      title="Ollama on Mac"
      description="Recommendations for Ollama and Mac"
      link="/docs/guides/local-models/ollama-mac"
    />
    <Card 
      title="Ollama on Windows"
      description="Recommendations for Ollama and Windows"
      link="/docs/guides/local-models/ollama-windows"
    />
    <Card 
      title="Ollama on Linux"
      description="Recommendations for Ollama and Linux"
      link="/docs/guides/local-models/ollama-linux"
    />
  </div>
</div>
